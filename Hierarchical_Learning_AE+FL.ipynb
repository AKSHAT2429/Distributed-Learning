{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y-lIQ6Tpu1r"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adz-5zIjpysa"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0NDe0tyv9AE"
      },
      "outputs": [],
      "source": [
        "input_shape = (28,28,1)\n",
        "\n",
        "x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "x_train=x_train / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        "x_test=x_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFon7nHlui0q"
      },
      "outputs": [],
      "source": [
        "# y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
        "# y_test = tf.one_hot(y_test.astype(np.int32), depth=10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# latent_dim = 64 \n",
        "\n",
        "# class Autoencoder(Model):\n",
        "#   def __init__(self, latent_dim):\n",
        "#     super(Autoencoder, self).__init__()\n",
        "#     self.latent_dim = latent_dim   \n",
        "#     self.encoder = tf.keras.Sequential([\n",
        "#       layers.Flatten(),\n",
        "#       layers.Dense(128, activation='relu'),\n",
        "#       layers.Dense(latent_dim, activation='relu'),\n",
        "#     ])\n",
        "#     self.decoder = tf.keras.Sequential([\n",
        "#       layers.Dense(128, activation='relu'),\n",
        "#       layers.Dense(784, activation='sigmoid'),\n",
        "#       layers.Reshape((28,28,1))\n",
        "#     ])\n",
        "\n",
        "#   def call(self, x):\n",
        "#     encoded = self.encoder(x)\n",
        "#     decoded = self.decoder(encoded)\n",
        "#     return decoded\n",
        "  \n",
        "# autoencoder = Autoencoder(latent_dim) "
      ],
      "metadata": {
        "id": "EBbZI5qjnfBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pguzYG80pyv5"
      },
      "outputs": [],
      "source": [
        "latent_dim = 64\n",
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim   \n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Conv2D(16, (3, 3), activation='relu', padding='same', input_shape=(28,28,1)),\n",
        "      layers.MaxPooling2D((2, 2), padding='same'),\n",
        "      layers.Conv2D(8, (3, 3), activation='relu', padding='same'),   \n",
        "      layers.MaxPooling2D((2, 2), padding='same'),\n",
        "      layers.Conv2D(4, (3, 3), activation='relu', padding='same'), \n",
        "      layers.MaxPooling2D((2, 2), padding='same'),\n",
        "       \n",
        "      # layers.BatchNormalization(),    \n",
        "      # layers.UpSampling2D(),\n",
        "      # layers.Conv2D(32, kernel_size=3, strides=1, padding='same', activation='relu'),   \n",
        "      # layers.BatchNormalization(),\n",
        "      # layers.Conv2D(1,  kernel_size=32,padding='same', activation='sigmoid'),\n",
        "      \n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      # layers.Conv2D(1, kernel_size=32, activation='relu', padding='same'),\n",
        "      layers.Conv2D(4, (3, 3), activation='relu', padding='same'),\n",
        "      layers.UpSampling2D((2, 2)),\n",
        "      layers.Conv2D(8, (3, 3), activation='relu', padding='same'),\n",
        "      layers.UpSampling2D((2, 2)),\n",
        "      layers.Conv2D(16, (3, 3), activation='relu'),\n",
        "      layers.UpSampling2D((2, 2)), \n",
        "      layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same'),\n",
        "      layers.Reshape((28,28,1))\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder = Autoencoder(latent_dim) "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "num_classes = 10\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "8KALXD3g4X5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for m in range(20,21,2):\n",
        "  num_nodes = m\n",
        "  each_dataset = int(x_train.shape[0]/num_nodes)\n",
        "  print(each_dataset)\n",
        "  x_train_list = []\n",
        "  history_list = []\n",
        "  autoencoder_list = []\n",
        "  encoded_images_train = []\n",
        "  decoded_images_train = []\n",
        "  encoded_images_test = []\n",
        "  decoded_images_test = []\n",
        "\n",
        "  for i in range(num_nodes):\n",
        "    x_train_list.append(x_train[int(i*each_dataset):int((i+1)*each_dataset)])\n",
        "\n",
        "    autoencoder_list.append(Autoencoder(latent_dim)) \n",
        "    autoencoder_list[i].compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "    print('Node Number',i)\n",
        "    autoencoder_list[i].fit(x_train_list[i], x_train_list[i],\n",
        "                  epochs=10,batch_size = 32,shuffle=True)\n",
        "    \n",
        "    encoded_images_train.append(autoencoder_list[i].encoder(x_train_list[i]).numpy())\n",
        "    # decoded_images_train.append(autoencoder_list[i]r.decoder(encoded_images_train[i]).numpy())\n",
        "    # encoded_images_test.append(autoencoder_list[i].encoder(x_test).numpy())\n",
        "    # decoded_images_test.append(autoencoder_list[i].decoder(encoded_images_test[i]).numpy())\n",
        "\n",
        "    n = 10\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for j in range(n):\n",
        "      # display original\n",
        "      ax = plt.subplot(2, n, j + 1)\n",
        "      plt.imshow(x_test[j,:,:,0])\n",
        "      plt.title(\"original\")\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "\n",
        "      # display reconstruction\n",
        "      ax = plt.subplot(2, n, j + 1 + n)\n",
        "      plt.imshow(encoded_images_test[i].reshape(encoded_images_test[i].shape[0],8,8,1)[j,:,:,0])\n",
        "      plt.title(\"reconstructed\")\n",
        "      plt.gray()\n",
        "      ax.get_xaxis().set_visible(False)\n",
        "      ax.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "  overall_encoded_train = np.zeros((x_train.shape[0],8,8,1))\n",
        "  # overall_encoded_test = np.zeros((x_test.shape[0],8,8,1))\n",
        "\n",
        "  for k in range(num_nodes):\n",
        "    overall_encoded_train[int(k*each_dataset):int((k+1)*each_dataset)] = encoded_images_train[k].reshape(encoded_images_train[k].shape[0],8,8,1)\n",
        "\n",
        "  overall_encoded_train = overall_encoded_train.reshape((overall_encoded_train.shape[0], 8,8, 1))\n",
        "\n",
        "  # model2 = tf.keras.models.Sequential([\n",
        "  #     tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=(8,8,1)),\n",
        "  #     tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n",
        "  #     tf.keras.layers.MaxPool2D(),\n",
        "  #     tf.keras.layers.Dropout(0.25),\n",
        "  #     tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "  #     tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "  #     tf.keras.layers.MaxPool2D(strides=(2,2)),\n",
        "  #     tf.keras.layers.Dropout(0.25),\n",
        "  #     tf.keras.layers.Flatten(),\n",
        "  #     tf.keras.layers.Dense(128, activation='relu'),\n",
        "  #     tf.keras.layers.Dropout(0.5),\n",
        "  #     tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  # ])\n",
        "\n",
        "  # model2.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "  # history = model2.fit(overall_encoded_train, y_train,\n",
        "  #                     batch_size=batch_size,\n",
        "  #                     epochs=20,\n",
        "  #                     validation_split=0.1)\n",
        "\n",
        "  # for l in range(num_nodes):\n",
        "  #   test_loss, test_acc = model2.evaluate(encoded_images_test[l].reshape(10000,8,8,1), y_test)\n",
        "  # saving_dict[m] = history.history"
      ],
      "metadata": {
        "id": "HWLxgvS74X8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('overall_encoded_train_20.txt','wb') as fp:\n",
        "  pickle.dump(overall_encoded_train,fp)"
      ],
      "metadata": {
        "id": "E_6TfQ50wnwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/stijani/tutorial"
      ],
      "metadata": {
        "id": "0o3fFmTcsg61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd tutorial"
      ],
      "metadata": {
        "id": "fwmQTV3QtGoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import cv2\n",
        "import os\n",
        "from imutils import paths\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import MaxPool2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "from fl_mnist_implementation_tutorial_utils import *"
      ],
      "metadata": {
        "id": "2vJImSydrWd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mnist = tf.keras.datasets.mnist\n",
        "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "# # idx = np.argsort(y_train)\n",
        "# # x_train = x_train[idx]\n",
        "# # y_train = y_train[idx]\n",
        "# x = np.concatenate((x_train, x_test))\n",
        "# y = np.concatenate((y_train, y_test))\n",
        "\n",
        "# train_size = 0.7\n",
        "X_train, X_test, y_train, y_test = train_test_split(overall_encoded_train, y_train, test_size=0.15, random_state=42)\n",
        "# X_train = overall_encoded_train\n",
        "# X_test = overall_encoded_test\n",
        "# idx = np.argsort(y_train)\n",
        "# X_train = X_train[idx]\n",
        "# y_train = y_train[idx]\n",
        "\n",
        "input_shape = (8,8, 1)\n",
        "\n",
        "X_train=X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
        "# X_train=X_train / 255.0\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
        "# X_test=X_test/255.0\n",
        "# y_test = np.concatenate((y_test, np.tile(y_test,19)))\n",
        "y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
        "y_test = tf.one_hot(y_test.astype(np.int32), depth=10)"
      ],
      "metadata": {
        "id": "n2K5ox_DtQzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients = create_clients(X_train, y_train, num_clients=2, initial='client')"
      ],
      "metadata": {
        "id": "G_PHwv6-uh9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clients_batched = dict()\n",
        "for (client_name, data) in clients.items():\n",
        "    clients_batched[client_name] = batch_data(data)\n",
        "    \n",
        "#process and batch the test set  \n",
        "test_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))"
      ],
      "metadata": {
        "id": "HevnPvAdupUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10\n",
        "input_shape = (8,8,1)"
      ],
      "metadata": {
        "id": "bBM8WesUwgKK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleMLP:\n",
        "    @staticmethod\n",
        "    def build(classes):\n",
        "        model = Sequential()\n",
        "        # model.add(Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape))\n",
        "        # model.add(MaxPool2D())\n",
        "        # model.add(Dropout(0.25))\n",
        "        # model.add(Flatten())\n",
        "        # model.add(Dense(128, activation='relu'))\n",
        "        # model.add(Dropout(0.5))\n",
        "        # model.add(Dense(classes, activation='softmax'))\n",
        "\n",
        "        model.add(Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape))\n",
        "        model.add(Conv2D(32, (5,5), padding='same', activation='relu'))\n",
        "        model.add(MaxPool2D())\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
        "        model.add(Conv2D(64, (5,5), padding='same', activation='relu'))\n",
        "        model.add(MaxPool2D(strides=(2,2)))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(512, activation='relu'))\n",
        "        model.add(Dropout(0.25))\n",
        "        model.add(Dense(classes, activation='softmax'))\n",
        "        return model"
      ],
      "metadata": {
        "id": "DvQPHpXHuqrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.01 \n",
        "comms_round = 50\n",
        "loss='categorical_crossentropy'\n",
        "metrics = ['accuracy']\n",
        "# optimizer=tf.keras.optimizers.Adam()\n",
        "optimizer = SGD(learning_rate=lr, \n",
        "                decay=lr / comms_round, \n",
        "                momentum=0.9\n",
        "               )            "
      ],
      "metadata": {
        "id": "iI7HqGhQutqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list3 = []\n",
        "list4 = []"
      ],
      "metadata": {
        "id": "acLyN0udFnDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smlp_global = SimpleMLP()\n",
        "global_model = smlp_global.build(10)\n",
        "        \n",
        "#commence global training loop\n",
        "for comm_round in range(comms_round):\n",
        "            \n",
        "    # get the global model's weights - will serve as the initial weights for all local models\n",
        "    global_weights = global_model.get_weights()\n",
        "    \n",
        "    #initial list to collect local model weights after scalling\n",
        "    scaled_local_weight_list = list()\n",
        "\n",
        "    #randomize client data - using keys\n",
        "    client_names= list(clients_batched.keys())\n",
        "    random.shuffle(client_names)\n",
        "    \n",
        "    #loop through each client and create new local model\n",
        "    for client in client_names:\n",
        "        smlp_local = SimpleMLP()\n",
        "        local_model = smlp_local.build(10)\n",
        "        local_model.compile(loss=loss, \n",
        "                      optimizer=optimizer, \n",
        "                      metrics=metrics)\n",
        "        \n",
        "        #set local model weight to the weight of the global model\n",
        "        local_model.set_weights(global_weights)\n",
        "        \n",
        "        #fit local model with client's data\n",
        "        history = local_model.fit(clients_batched[client], epochs=1)\n",
        "        \n",
        "        #scale the model weights and add to list\n",
        "        scaling_factor = weight_scalling_factor(clients_batched, client)\n",
        "        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n",
        "        scaled_local_weight_list.append(scaled_weights)\n",
        "        \n",
        "        #clear session to free memory after each communication round\n",
        "        K.clear_session()\n",
        "        \n",
        "    #to get the average over all the local model, we simply take the sum of the scaled weights\n",
        "    average_weights = sum_scaled_weights(scaled_local_weight_list)\n",
        "    \n",
        "    #update global model \n",
        "    global_model.set_weights(average_weights)\n",
        "\n",
        "    #test global model and print out metrics after each communications round\n",
        "    for(X_test, Y_test) in test_batched:\n",
        "        global_acc, global_loss = test_model(X_test, y_test, global_model, comm_round)\n",
        "\n",
        "        # print(Y_test)\n",
        "        list3.append(global_acc)\n",
        "        list4.append(global_loss)"
      ],
      "metadata": {
        "id": "PnB34oOcuyt8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}