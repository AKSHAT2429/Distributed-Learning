{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0y-lIQ6Tpu1r"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras import layers, losses\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adz-5zIjpysa"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0NDe0tyv9AE"
      },
      "outputs": [],
      "source": [
        "input_shape = (28,28,1)\n",
        "\n",
        "x_train=x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
        "x_train=x_train / 255.0\n",
        "x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], x_test.shape[2], 1)\n",
        "x_test=x_test/255.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y_train = np.array([y_train[i][0] for i in range(len(y_train))])\n",
        "# y_test = np.array([y_test[i][0] for i in range(len(y_test))])"
      ],
      "metadata": {
        "id": "CIHnT6YZsMkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFon7nHlui0q"
      },
      "outputs": [],
      "source": [
        "y_train = tf.one_hot(y_train.astype(np.int32), depth=10)\n",
        "y_test = tf.one_hot(y_test.astype(np.int32), depth=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pguzYG80pyv5"
      },
      "outputs": [],
      "source": [
        "latent_dim = 64 \n",
        "\n",
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim   \n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      layers.Flatten(),\n",
        "      layers.Dense(64, activation='relu'),\n",
        "    ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      layers.Dense(784, activation='sigmoid'),\n",
        "      layers.Reshape((28,28,1))\n",
        "    ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "  \n",
        "autoencoder = Autoencoder(latent_dim) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wogK42M6vI24"
      },
      "source": [
        "**CNN**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNKyYnisqaJ_"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_classes = 10\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEQ7kbOBvX_y"
      },
      "outputs": [],
      "source": [
        "input_shape = (28,28,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "181_9qsOqaMG"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=(28,28,1)),\n",
        "    tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "    tf.keras.layers.MaxPool2D(strides=(2,2)),\n",
        "    tf.keras.layers.Dropout(0.25),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics='accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWuJf82qw9Vy"
      },
      "outputs": [],
      "source": [
        "history = model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=40)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "saving_dict = {}\n",
        "saving_dict[10] = history.history"
      ],
      "metadata": {
        "id": "fWTJeIr_qMrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open('central.txt','wb') as fp:\n",
        "  pickle.dump(saving_dict,fp)"
      ],
      "metadata": {
        "id": "-acHcUWOqU_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWUoYP0Cw_Lj"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbY7Roo2VJ0r"
      },
      "source": [
        "Peer to Peer Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsIdgY8qigUN"
      },
      "outputs": [],
      "source": [
        "for m in range(2,11,2):\n",
        "  num_nodes = m\n",
        "  each_dataset = int(x_train.shape[0]/num_nodes)\n",
        "  x_train_list = []\n",
        "  history_list = []\n",
        "  autoencoder_list = []\n",
        "  encoded_images_train = []\n",
        "  decoded_images_train = []\n",
        "  encoded_images_test = []\n",
        "  decoded_images_test = []\n",
        "\n",
        "  for i in range(num_nodes):\n",
        "    x_train_list.append(x_train[int(i*each_dataset):int((i+1)*each_dataset)])\n",
        "\n",
        "    autoencoder_list.append(Autoencoder(latent_dim)) \n",
        "    autoencoder_list[i].compile(optimizer='adam', loss=losses.MeanSquaredError())\n",
        "\n",
        "    autoencoder_list[i].fit(x_train_list[i], x_train_list[i],\n",
        "                  epochs=20,\n",
        "                  shuffle=True,\n",
        "                  validation_data=(x_test, x_test))\n",
        "    \n",
        "    encoded_images_train.append(autoencoder_list[i].encoder(x_train_list[i]).numpy())\n",
        "    # decoded_images_train.append(autoencoder_list[i]r.decoder(encoded_images_train[i]).numpy())\n",
        "    encoded_images_test.append(autoencoder_list[i].encoder(x_test).numpy())\n",
        "    # decoded_images_test.append(autoencoder_list[i].decoder(encoded_images_test[i]).numpy())\n",
        "\n",
        "    n = 10\n",
        "    plt.figure(figsize=(20, 4))\n",
        "    for j in range(n):\n",
        "      # display original\n",
        "        ax = plt.subplot(2, n, j + 1)\n",
        "        plt.imshow(x_test[j,:,:,0])\n",
        "        plt.title(\"original\")\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "\n",
        "        # display reconstruction\n",
        "        ax = plt.subplot(2, n, j + 1 + n)\n",
        "        plt.imshow(encoded_images_test[i].reshape(encoded_images_test[i].shape[0],8,8,1)[j,:,:,0])\n",
        "        plt.title(\"reconstructed\")\n",
        "        plt.gray()\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.get_yaxis().set_visible(False)\n",
        "    plt.show()\n",
        "  overall_encoded_train = np.zeros((x_train.shape[0],8,8,1))\n",
        "\n",
        "  for k in range(num_nodes):\n",
        "    overall_encoded_train[int(k*each_dataset):int((k+1)*each_dataset)] = encoded_images_train[k].reshape(encoded_images_train[k].shape[0],8,8,1)\n",
        "\n",
        "  overall_encoded_train = overall_encoded_train.reshape((overall_encoded_train.shape[0], 8,8, 1))\n",
        "\n",
        "  model2 = tf.keras.models.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu', input_shape=(8,8,1)),\n",
        "      tf.keras.layers.Conv2D(32, (5,5), padding='same', activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "      tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "      tf.keras.layers.Conv2D(64, (3,3), padding='same', activation='relu'),\n",
        "      tf.keras.layers.MaxPool2D(strides=(2,2)),\n",
        "      tf.keras.layers.Dropout(0.25),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(128, activation='relu'),\n",
        "      tf.keras.layers.Dropout(0.5),\n",
        "      tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model2.compile(optimizer=tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "  history = model2.fit(overall_encoded_train, y_train,\n",
        "                      batch_size=batch_size,\n",
        "                      epochs=20,\n",
        "                      validation_split=0.1)\n",
        "\n",
        "  for l in range(num_nodes):\n",
        "    test_loss, test_acc = model2.evaluate(encoded_images_test[l].reshape(10000,8,8,1), y_test)\n",
        "  saving_dict[m] = history.history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXPS0b49xeO3"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('ae.txt','wb') as fp:\n",
        "  pickle.dump(saving_dict,fp)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}