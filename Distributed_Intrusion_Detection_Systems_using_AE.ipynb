{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Loading","metadata":{}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd\nimport warnings\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport matplotlib.pyplot as plt\nimport time\nfrom sklearn.metrics import accuracy_score\n\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-15T12:10:07.299631Z","iopub.execute_input":"2022-11-15T12:10:07.300355Z","iopub.status.idle":"2022-11-15T12:10:07.306895Z","shell.execute_reply.started":"2022-11-15T12:10:07.300313Z","shell.execute_reply":"2022-11-15T12:10:07.305716Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"setting cols","metadata":{}},{"cell_type":"code","source":"cols = [' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n       ' Total Backward Packets', 'Total Length of Fwd Packets',\n       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',' Label']","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:09.159287Z","iopub.execute_input":"2022-11-15T12:10:09.160312Z","iopub.status.idle":"2022-11-15T12:10:09.168939Z","shell.execute_reply.started":"2022-11-15T12:10:09.160264Z","shell.execute_reply":"2022-11-15T12:10:09.167808Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"import dataset","metadata":{}},{"cell_type":"code","source":"df1=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\", usecols = cols)#,nrows = 50000\ndf2=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\", usecols = cols)\ndf3=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Friday-WorkingHours-Morning.pcap_ISCX.csv\", usecols = cols)\ndf4=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\", usecols = cols)\ndf5=pd.read_csv(\"/kaggle/input/cicids2017/MachineLearningCSV/MachineLearningCVE/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\", usecols = cols)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:09.556926Z","iopub.execute_input":"2022-11-15T12:10:09.557280Z","iopub.status.idle":"2022-11-15T12:10:22.468681Z","shell.execute_reply.started":"2022-11-15T12:10:09.557250Z","shell.execute_reply":"2022-11-15T12:10:22.467653Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"combine data","metadata":{}},{"cell_type":"code","source":"df = pd.concat([df1,df2])\ndel df1,df2\ndf = pd.concat([df,df3])\ndel df3\ndf = pd.concat([df,df4])\ndel df4\ndf = pd.concat([df,df5])\ndel df5","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:22.470553Z","iopub.execute_input":"2022-11-15T12:10:22.471205Z","iopub.status.idle":"2022-11-15T12:10:23.341507Z","shell.execute_reply.started":"2022-11-15T12:10:22.471175Z","shell.execute_reply":"2022-11-15T12:10:23.340417Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Machine Learning Model LinearRegression","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score,classification_report\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import layers, losses\nfrom tensorflow.keras.datasets import fashion_mnist\nfrom tensorflow.keras.models import Model","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:23.343041Z","iopub.execute_input":"2022-11-15T12:10:23.343495Z","iopub.status.idle":"2022-11-15T12:10:33.577309Z","shell.execute_reply.started":"2022-11-15T12:10:23.343432Z","shell.execute_reply":"2022-11-15T12:10:33.576228Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"creating data","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import confusion_matrix\ndata = df.copy()\ndata.dropna(1,inplace=True)\n#LinearRegression doesn't use inf,NaN datas. Flow Packets/s includes that so reduce it.\ndata=data.drop(columns=[' Flow Packets/s'], axis=1, inplace=False)\n\n#make function to measure the mean\n# def testing_all(data):\nx = data[data.columns[0:-1]]\ny = data[[' Label']]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nx = scaler.fit_transform(x)\n# valid_df[col] = scaler.transform(valid_df[col])\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,train_size=0.8,test_size=0.2)\nattack = ['DDoS', 'PortScan', 'Bot', 'Infiltration', 'Web Attack � Brute Force', 'Web Attack � XSS', 'Web Attack � Sql Injection']\nnormal = 'BENIGN'\ny_train=y_train.replace(attack,0)\ny_train=y_train.replace(normal,1)\ny_test=y_test.replace(attack,0)\ny_test=y_test.replace(normal,1)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n# y_train = np.array([y_train[i][0] for i in range(len(y_train))])\n# y_test = np.array([y_test[i][0] for i in range(len(y_test))])\n# y_train = tf.one_hot(y_train.astype(np.int32), depth=2)\n# y_test = tf.one_hot(y_test.astype(np.int32), depth=2)\n#     model=LinearRegression()\n#     model.fit(x_train, y_train) \n#     y_pred = model.predict(x_test)\n#     #Relabel values within a certain range to measure values\n#     for i in range(0,y_pred.size):\n#         if(y_pred[i]>0):\n#             y_pred[i]=1\n#         else:\n#             y_pred[i]=-1\n#     cf_matrix = confusion_matrix(y_test, y_pred)\n#     tn, fp, fn, tp = cf_matrix.ravel()\n#     recall = tp/(tp+fn) #공격 적발률\n#     precision = tp/(tp+fp) #정상 이용자 판단 확률\n# #    print(\"Recall\", recall, \"\\nPrecision\", precision)\n#     f = 2 * (precision*recall)/(precision+recall)\n# #    print(\"F1 Score\", f)\n    \n#     return f","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:33.580317Z","iopub.execute_input":"2022-11-15T12:10:33.581267Z","iopub.status.idle":"2022-11-15T12:10:38.391506Z","shell.execute_reply.started":"2022-11-15T12:10:33.581226Z","shell.execute_reply":"2022-11-15T12:10:38.390373Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"latent_dim = 64 \n\nclass Autoencoder(Model):\n  def __init__(self, latent_dim):\n    super(Autoencoder, self).__init__()\n    self.latent_dim = latent_dim   \n    self.encoder = tf.keras.Sequential([\n      layers.Flatten(),\n      layers.Dense(64, activation='relu'),\n    ])\n    self.decoder = tf.keras.Sequential([\n      layers.Dense(76, activation='sigmoid'),\n      layers.Reshape((76,))\n    ])\n\n  def call(self, x):\n    encoded = self.encoder(x)\n    decoded = self.decoder(encoded)\n    return decoded\n  \nautoencoder = Autoencoder(latent_dim) ","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:38.392986Z","iopub.execute_input":"2022-11-15T12:10:38.394017Z","iopub.status.idle":"2022-11-15T12:10:44.312392Z","shell.execute_reply.started":"2022-11-15T12:10:38.393975Z","shell.execute_reply":"2022-11-15T12:10:44.311274Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2022-11-15 12:10:38.567889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:38.568888: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:38.866640: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:38.867569: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:38.868614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:38.869578: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:38.871793: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-11-15 12:10:39.110601: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:39.111626: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:39.112523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:39.113278: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:39.114070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:39.114797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:43.850759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:43.851994: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:43.853070: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:43.854183: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:43.855236: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:43.856311: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13789 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n2022-11-15 12:10:43.862173: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-11-15 12:10:43.863231: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13789 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n","output_type":"stream"}]},{"cell_type":"code","source":"x_train = np.array(x_train)\nx_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:44.313947Z","iopub.execute_input":"2022-11-15T12:10:44.314876Z","iopub.status.idle":"2022-11-15T12:10:44.532590Z","shell.execute_reply.started":"2022-11-15T12:10:44.314818Z","shell.execute_reply":"2022-11-15T12:10:44.531307Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(76,)"},"metadata":{}}]},{"cell_type":"code","source":"x_test = np.array(x_test)\nx_test[0].shape","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:44.534490Z","iopub.execute_input":"2022-11-15T12:10:44.534946Z","iopub.status.idle":"2022-11-15T12:10:44.598271Z","shell.execute_reply.started":"2022-11-15T12:10:44.534904Z","shell.execute_reply":"2022-11-15T12:10:44.597089Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"(76,)"},"metadata":{}}]},{"cell_type":"code","source":"y_test","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:10:44.599822Z","iopub.execute_input":"2022-11-15T12:10:44.600937Z","iopub.status.idle":"2022-11-15T12:10:44.609214Z","shell.execute_reply.started":"2022-11-15T12:10:44.600896Z","shell.execute_reply":"2022-11-15T12:10:44.608064Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"array([[1],\n       [1],\n       [1],\n       ...,\n       [1],\n       [1],\n       [0]])"},"metadata":{}}]},{"cell_type":"code","source":"for m in range(2,11,2):\n    num_nodes = m\n    each_dataset = int(x_train.shape[0]/num_nodes)\n    x_train_list = []\n    history_list = []\n    autoencoder_list = []\n    encoded_images_train = []\n    decoded_images_train = []\n    encoded_images_test = []\n    decoded_images_test = []\n\n    for i in range(num_nodes):\n        x_train_list.append(x_train[int(i*each_dataset):int((i+1)*each_dataset)])\n\n        autoencoder_list.append(Autoencoder(latent_dim)) \n        autoencoder_list[i].compile(optimizer='adam', loss=losses.MeanSquaredError())\n\n        autoencoder_list[i].fit(x_train_list[i], x_train_list[i],\n                  epochs=1,\n                  shuffle=True)\n    \n        encoded_images_train.append(autoencoder_list[i].encoder(x_train_list[i]).numpy())\n    # decoded_images_train.append(autoencoder_list[i]r.decoder(encoded_images_train[i]).numpy())\n        encoded_images_test.append(autoencoder_list[i].encoder(x_test).numpy())\n    # decoded_images_test.append(autoencoder_list[i].decoder(encoded_images_test[i]).numpy())\n\n#     n = 10\n#     plt.figure(figsize=(20, 4))\n#     for j in range(n):\n#       # display original\n#       ax = plt.subplot(2, n, j + 1)\n#       plt.imshow(x_test[j,:,:,0])\n#       plt.title(\"original\")\n#       plt.gray()\n#       ax.get_xaxis().set_visible(False)\n#       ax.get_yaxis().set_visible(False)\n\n#       # display reconstruction\n#       ax = plt.subplot(2, n, j + 1 + n)\n#       plt.imshow(encoded_images_test[i].reshape(encoded_images_test[i].shape[0],8,8,1)[j,:,:,0])\n#       plt.title(\"reconstructed\")\n#       plt.gray()\n#       ax.get_xaxis().set_visible(False)\n#       ax.get_yaxis().set_visible(False)\n#     plt.show()\n    overall_encoded_train = np.zeros((x_train.shape[0],64))\n    overall_encoded_test = np.zeros((x_test.shape[0],64))\n#     print(x_train.shape[0])\n#     print(encoded_images_train.shape)\n    for k in range(num_nodes):\n        overall_encoded_train[int(k*each_dataset):int((k+1)*each_dataset)] = encoded_images_train[k].reshape(encoded_images_train[k].shape[0],64)\n\n    overall_encoded_train = overall_encoded_train.reshape((overall_encoded_train.shape[0], 64))\n#     model=LinearRegression()\n#     model.fit(overall_encoded_train, y_train) \n    \n#     for l in range(num_nodes):\n#         y_pred = model.predict(encoded_images_test[l])\n#     #Relabel values within a certain range to measure values\n#         for i in range(0,y_pred.size):\n#             if(y_pred[i]>0):\n#                 y_pred[i]=1\n#             else:\n#                 y_pred[i]=-1\n#         cf_matrix = confusion_matrix(y_test, y_pred)\n#     #     print(cf_matrix)\n#     #     print(cf_matrix.ravel())\n#     #     print(classification_report(y_test, y_pred))\n\n#         print(accuracy_score(y_test, y_pred))\n#     recall = tp/(tp+fn) \n#     precision = tp/(tp+fp) \n#     print(\"Recall\", recall, \"\\nPrecision\", precision)\n#     f = 2 * (precision*recall)/(precision+recall)\n#     print(\"F1 Score\", f)\n\n    model2 = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(48, input_shape=(64,)),\n    \n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(1, activation = 'sigmoid')\n    ])\n\n    model2.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n\n\n\n\n    history = model2.fit(overall_encoded_train, y_train,batch_size = 64,\n                      epochs=2)\n\n    for l in range(num_nodes):\n        test_loss, test_acc = model2.evaluate(encoded_images_test[l].reshape(encoded_images_test[l].shape[0],64), y_test)\n#     saving_dict[m] = history.history","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:20:51.183432Z","iopub.execute_input":"2022-11-15T12:20:51.183833Z","iopub.status.idle":"2022-11-15T12:41:59.598360Z","shell.execute_reply.started":"2022-11-15T12:20:51.183798Z","shell.execute_reply":"2022-11-15T12:41:59.597253Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"14528/14528 [==============================] - 29s 2ms/step - loss: 0.0020\n14528/14528 [==============================] - 29s 2ms/step - loss: 0.0017\nEpoch 1/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1406 - accuracy: 0.9372\nEpoch 2/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1201 - accuracy: 0.9437\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1034 - accuracy: 0.9489\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1076 - accuracy: 0.9460\n7264/7264 [==============================] - 15s 2ms/step - loss: 0.0033\n7264/7264 [==============================] - 15s 2ms/step - loss: 0.0033\n7264/7264 [==============================] - 14s 2ms/step - loss: 0.0034\n7264/7264 [==============================] - 15s 2ms/step - loss: 0.0034\nEpoch 1/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1639 - accuracy: 0.9217\nEpoch 2/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1373 - accuracy: 0.9339\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1136 - accuracy: 0.9450\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1130 - accuracy: 0.9467\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1137 - accuracy: 0.9469\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1129 - accuracy: 0.9444\n4843/4843 [==============================] - 10s 2ms/step - loss: 0.0050\n4843/4843 [==============================] - 10s 2ms/step - loss: 0.0052\n4843/4843 [==============================] - 10s 2ms/step - loss: 0.0051\n4843/4843 [==============================] - 10s 2ms/step - loss: 0.0052\n4843/4843 [==============================] - 10s 2ms/step - loss: 0.0054\n4843/4843 [==============================] - 10s 2ms/step - loss: 0.0050\nEpoch 1/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1811 - accuracy: 0.9153\nEpoch 2/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1476 - accuracy: 0.9323\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1306 - accuracy: 0.9358\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1237 - accuracy: 0.9391\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1334 - accuracy: 0.9361\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1245 - accuracy: 0.9529\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1263 - accuracy: 0.9405\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1243 - accuracy: 0.9463\n3632/3632 [==============================] - 7s 2ms/step - loss: 0.0065\n3632/3632 [==============================] - 7s 2ms/step - loss: 0.0065\n3632/3632 [==============================] - 8s 2ms/step - loss: 0.0063\n3632/3632 [==============================] - 7s 2ms/step - loss: 0.0068\n3632/3632 [==============================] - 7s 2ms/step - loss: 0.0065\n3632/3632 [==============================] - 8s 2ms/step - loss: 0.0065\n3632/3632 [==============================] - 8s 2ms/step - loss: 0.0067\n3632/3632 [==============================] - 7s 2ms/step - loss: 0.0065\nEpoch 1/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1965 - accuracy: 0.9084\nEpoch 2/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1566 - accuracy: 0.9281\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1295 - accuracy: 0.9385\n7264/7264 [==============================] - 14s 2ms/step - loss: 0.1371 - accuracy: 0.9359\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1609 - accuracy: 0.9169\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1393 - accuracy: 0.9341\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1430 - accuracy: 0.9308\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1313 - accuracy: 0.9249\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1329 - accuracy: 0.9336\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1690 - accuracy: 0.9189\n2906/2906 [==============================] - 7s 2ms/step - loss: 0.0088\n2906/2906 [==============================] - 6s 2ms/step - loss: 0.0083\n2906/2906 [==============================] - 6s 2ms/step - loss: 0.0085\n2906/2906 [==============================] - 6s 2ms/step - loss: 0.0082\n2906/2906 [==============================] - 6s 2ms/step - loss: 0.0082\n2906/2906 [==============================] - 6s 2ms/step - loss: 0.0082\n2906/2906 [==============================] - 6s 2ms/step - loss: 0.0080\n2906/2906 [==============================] - 7s 2ms/step - loss: 0.0082\n2906/2906 [==============================] - 6s 2ms/step - loss: 0.0088\n2906/2906 [==============================] - 6s 2ms/step - loss: 0.0083\nEpoch 1/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.2257 - accuracy: 0.8917\nEpoch 2/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1811 - accuracy: 0.9168\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1468 - accuracy: 0.9419\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1291 - accuracy: 0.9461\n7264/7264 [==============================] - 12s 2ms/step - loss: 0.1409 - accuracy: 0.9390\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1368 - accuracy: 0.9408\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1362 - accuracy: 0.9476\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1517 - accuracy: 0.9363\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1360 - accuracy: 0.9308\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1350 - accuracy: 0.9389\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1309 - accuracy: 0.9441\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.1342 - accuracy: 0.9379\n","output_type":"stream"}]},{"cell_type":"code","source":"model2 = tf.keras.models.Sequential([\n        tf.keras.layers.Dense(48, input_shape=(76,)),\n    \n        tf.keras.layers.Dense(32, activation='relu'),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(1, activation = 'sigmoid')\n    ])\n\nmodel2.compile(optimizer=tf.keras.optimizers.Adam(), loss=tf.keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n\n\n\n\nhistory = model2.fit(x_train, y_train,batch_size = 64,\n                  epochs=2)\n\ntest_loss, test_acc = model2.evaluate(x_test, y_test)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:42:54.544994Z","iopub.execute_input":"2022-11-15T12:42:54.545395Z","iopub.status.idle":"2022-11-15T12:44:30.989937Z","shell.execute_reply.started":"2022-11-15T12:42:54.545364Z","shell.execute_reply":"2022-11-15T12:44:30.988821Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch 1/2\n14528/14528 [==============================] - 38s 3ms/step - loss: 0.1215 - accuracy: 0.9519\nEpoch 2/2\n14528/14528 [==============================] - 37s 3ms/step - loss: 0.0988 - accuracy: 0.9607\n7264/7264 [==============================] - 13s 2ms/step - loss: 0.0808 - accuracy: 0.9642\n","output_type":"stream"}]},{"cell_type":"code","source":"print(test_acc)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T11:54:50.310667Z","iopub.execute_input":"2022-11-15T11:54:50.311056Z","iopub.status.idle":"2022-11-15T11:54:50.317551Z","shell.execute_reply.started":"2022-11-15T11:54:50.311022Z","shell.execute_reply":"2022-11-15T11:54:50.316542Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"0.7496891617774963\n","output_type":"stream"}]},{"cell_type":"code","source":"!git clone https://github.com/stijani/tutorial","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:44:45.045786Z","iopub.execute_input":"2022-11-15T12:44:45.046502Z","iopub.status.idle":"2022-11-15T12:44:47.386335Z","shell.execute_reply.started":"2022-11-15T12:44:45.046460Z","shell.execute_reply":"2022-11-15T12:44:47.385065Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"Cloning into 'tutorial'...\nremote: Enumerating objects: 7, done.\u001b[K\nremote: Counting objects: 100% (7/7), done.\u001b[K\nremote: Compressing objects: 100% (6/6), done.\u001b[K\nremote: Total 7 (delta 0), reused 2 (delta 0), pack-reused 0\u001b[K\nUnpacking objects: 100% (7/7), 4.90 KiB | 1.63 MiB/s, done.\n","output_type":"stream"}]},{"cell_type":"code","source":"cd tutorial","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:44:57.052279Z","iopub.execute_input":"2022-11-15T12:44:57.053050Z","iopub.status.idle":"2022-11-15T12:44:57.060772Z","shell.execute_reply.started":"2022-11-15T12:44:57.053009Z","shell.execute_reply":"2022-11-15T12:44:57.059599Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"/kaggle/working/tutorial\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport random\nimport cv2\nimport os\nfrom imutils import paths\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import accuracy_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import MaxPooling2D\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import MaxPool2D\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras import backend as K\n\nfrom fl_mnist_implementation_tutorial_utils import *","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:45:17.575093Z","iopub.execute_input":"2022-11-15T12:45:17.575521Z","iopub.status.idle":"2022-11-15T12:45:17.593094Z","shell.execute_reply.started":"2022-11-15T12:45:17.575478Z","shell.execute_reply":"2022-11-15T12:45:17.592063Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"!pip install imutils","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:45:01.305020Z","iopub.execute_input":"2022-11-15T12:45:01.305422Z","iopub.status.idle":"2022-11-15T12:45:17.571953Z","shell.execute_reply.started":"2022-11-15T12:45:01.305391Z","shell.execute_reply":"2022-11-15T12:45:17.570690Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Collecting imutils\n  Downloading imutils-0.5.4.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: imutils\n  Building wheel for imutils (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for imutils: filename=imutils-0.5.4-py3-none-any.whl size=25858 sha256=a3b50450c63ffbd69a0a3da1d879dfd90125e36aa032c55ca305bcfaf92c31b3\n  Stored in directory: /root/.cache/pip/wheels/86/d7/0a/4923351ed1cec5d5e24c1eaf8905567b02a0343b24aa873df2\nSuccessfully built imutils\nInstalling collected packages: imutils\nSuccessfully installed imutils-0.5.4\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"def load(paths, verbose=-1):\n    '''expects images for each class in seperate dir, \n    e.g all digits in 0 class in the directory named 0 '''\n    data = list()\n    labels = list()\n    # loop over the input images\n    for (i, imgpath) in enumerate(paths):\n        # load the image and extract the class labels\n        im_gray = cv2.imread(imgpath, cv2.IMREAD_GRAYSCALE)\n        image = np.array(im_gray).flatten()\n        label = imgpath.split(os.path.sep)[-2]\n        # scale the image to [0, 1] and add to list\n        data.append(image/255)\n        labels.append(label)\n        # show an update every `verbose` images\n        if verbose > 0 and i > 0 and (i + 1) % verbose == 0:\n            print(\"[INFO] processed {}/{}\".format(i + 1, len(paths)))\n    # return a tuple of the data and labels\n    return data, labels","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:45:17.595025Z","iopub.execute_input":"2022-11-15T12:45:17.595462Z","iopub.status.idle":"2022-11-15T12:45:17.605605Z","shell.execute_reply.started":"2022-11-15T12:45:17.595426Z","shell.execute_reply":"2022-11-15T12:45:17.604287Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"x = np.concatenate((x_train, x_test))\ny = np.concatenate((y_train, y_test))\n\ntrain_size = 0.7\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:45:28.710301Z","iopub.execute_input":"2022-11-15T12:45:28.711444Z","iopub.status.idle":"2022-11-15T12:45:29.318372Z","shell.execute_reply.started":"2022-11-15T12:45:28.711394Z","shell.execute_reply":"2022-11-15T12:45:29.317312Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def create_clients(image_list, label_list, num_clients=10, initial='clients'):\n    ''' return: a dictionary with keys clients' names and value as \n                data shards - tuple of images and label lists.\n        args: \n            image_list: a list of numpy arrays of training images\n            label_list:a list of binarized labels for each image\n            num_client: number of fedrated members (clients)\n            initials: the clients'name prefix, e.g, clients_1 \n            \n    '''\n\n    #create a list of client names\n    client_names = ['{}_{}'.format(initial, i+1) for i in range(num_clients)]\n\n    #randomize the data\n    data = list(zip(image_list, label_list))\n\n    #shard data and place at each client\n    size = len(data)//num_clients\n    shards = [data[i:i + size] for i in range(0, size*num_clients, size)]\n\n    #number of clients must equal number of shards\n    assert(len(shards) == len(client_names))\n\n    return {client_names[i] : shards[i] for i in range(len(client_names))} ","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:45:32.357257Z","iopub.execute_input":"2022-11-15T12:45:32.357635Z","iopub.status.idle":"2022-11-15T12:45:32.365302Z","shell.execute_reply.started":"2022-11-15T12:45:32.357604Z","shell.execute_reply":"2022-11-15T12:45:32.364221Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"clients = create_clients(X_train, y_train, num_clients=10, initial='client')","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:57:14.409861Z","iopub.execute_input":"2022-11-15T12:57:14.410354Z","iopub.status.idle":"2022-11-15T12:57:15.272656Z","shell.execute_reply.started":"2022-11-15T12:57:14.410312Z","shell.execute_reply":"2022-11-15T12:57:15.271551Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"\ndef batch_data(data_shard, bs=32):\n    '''Takes in a clients data shard and create a tfds object off it\n    args:\n        shard: a data, label constituting a client's data shard\n        bs:batch size\n    return:\n        tfds object'''\n    #seperate shard into data and labels lists\n    data, label = zip(*data_shard)\n    dataset = tf.data.Dataset.from_tensor_slices((list(data), list(label)))\n    return dataset.shuffle(len(label)).batch(bs)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:57:15.275650Z","iopub.execute_input":"2022-11-15T12:57:15.276594Z","iopub.status.idle":"2022-11-15T12:57:15.295527Z","shell.execute_reply.started":"2022-11-15T12:57:15.276563Z","shell.execute_reply":"2022-11-15T12:57:15.294463Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"clients_batched = dict()\nfor (client_name, data) in clients.items():\n    clients_batched[client_name] = batch_data(data)\n    \n#process and batch the test set  \ntest_batched = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(len(y_test))","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:57:15.299092Z","iopub.execute_input":"2022-11-15T12:57:15.299442Z","iopub.status.idle":"2022-11-15T12:58:10.066929Z","shell.execute_reply.started":"2022-11-15T12:57:15.299413Z","shell.execute_reply":"2022-11-15T12:58:10.065874Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"class SimpleMLP:\n    @staticmethod\n    def build(classes):\n        model = Sequential()\n        # model.add(Conv2D(32, (5,5), padding='same', activation='relu', input_shape=input_shape))\n        # model.add(MaxPool2D())\n        # model.add(Dropout(0.25))\n        # model.add(Flatten())\n        # model.add(Dense(128, activation='relu'))\n        # model.add(Dropout(0.5))\n        # model.add(Dense(classes, activation='softmax'))\n        model.add(Dense(48, input_shape=(76,)))\n        model.add(Dense(128, activation='relu'))\n        model.add(Dropout(0.5))\n        model.add(Dense(1, activation='sigmoid'))\n        return model","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:58:10.069409Z","iopub.execute_input":"2022-11-15T12:58:10.070784Z","iopub.status.idle":"2022-11-15T12:58:10.077340Z","shell.execute_reply.started":"2022-11-15T12:58:10.070744Z","shell.execute_reply":"2022-11-15T12:58:10.076463Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"lr = 0.001 \ncomms_round = 2\nloss=tf.keras.losses.BinaryCrossentropy()\nmetrics = ['accuracy']\noptimizer = tf.keras.optimizers.Adam()          ","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:58:10.078897Z","iopub.execute_input":"2022-11-15T12:58:10.079658Z","iopub.status.idle":"2022-11-15T12:58:10.092789Z","shell.execute_reply.started":"2022-11-15T12:58:10.079596Z","shell.execute_reply":"2022-11-15T12:58:10.091896Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def weight_scalling_factor(clients_trn_data, client_name):\n    client_names = list(clients_trn_data.keys())\n    #get the bs\n    bs = list(clients_trn_data[client_name])[0][0].shape[0]\n    #first calculate the total training data points across clinets\n    global_count = sum([tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy() for client_name in client_names])*bs\n    # get the total number of data points held by a client\n    local_count = tf.data.experimental.cardinality(clients_trn_data[client_name]).numpy()*bs\n    return local_count/global_count\n\n\ndef scale_model_weights(weight, scalar):\n    '''function for scaling a models weights'''\n    weight_final = []\n    steps = len(weight)\n    for i in range(steps):\n        weight_final.append(scalar * weight[i])\n    return weight_final\n\n\n\ndef sum_scaled_weights(scaled_weight_list):\n    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n    avg_grad = list()\n    #get the average grad accross all client gradients\n    for grad_list_tuple in zip(*scaled_weight_list):\n        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n        avg_grad.append(layer_mean)\n        \n    return avg_grad\n\n\ndef test_model(X_test, Y_test,  model, comm_round):\n    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n    #logits = model.predict(X_test, batch_size=100)\n    logits = model.predict(X_test)\n    loss = cce(Y_test, logits)\n    acc = accuracy_score(tf.argmax(logits, axis=1), tf.argmax(Y_test, axis=1))\n    print('comm_round: {} | global_acc: {:.3%} | global_loss: {}'.format(comm_round, acc, loss))\n    return acc, loss","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:58:10.094107Z","iopub.execute_input":"2022-11-15T12:58:10.094546Z","iopub.status.idle":"2022-11-15T12:58:10.107769Z","shell.execute_reply.started":"2022-11-15T12:58:10.094509Z","shell.execute_reply":"2022-11-15T12:58:10.106895Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"smlp_global = SimpleMLP()\nglobal_model = smlp_global.build(10)\n        \n#commence global training loop\nfor comm_round in range(comms_round):\n            \n    # get the global model's weights - will serve as the initial weights for all local models\n    global_weights = global_model.get_weights()\n    \n    #initial list to collect local model weights after scalling\n    scaled_local_weight_list = list()\n\n    #randomize client data - using keys\n    client_names= list(clients_batched.keys())\n    random.shuffle(client_names)\n    \n    #loop through each client and create new local model\n    for client in client_names:\n        smlp_local = SimpleMLP()\n        local_model = smlp_local.build(10)\n        local_model.compile(loss=loss, \n                      optimizer=optimizer, \n                      metrics=metrics)\n        \n        #set local model weight to the weight of the global model\n        local_model.set_weights(global_weights)\n        \n        #fit local model with client's data\n        local_model.fit(clients_batched[client], epochs=1, verbose=0)\n        \n        #scale the model weights and add to list\n        scaling_factor = weight_scalling_factor(clients_batched, client)\n        scaled_weights = scale_model_weights(local_model.get_weights(), scaling_factor)\n        scaled_local_weight_list.append(scaled_weights)\n        \n        #clear session to free memory after each communication round\n        K.clear_session()\n        \n    #to get the average over all the local model, we simply take the sum of the scaled weights\n    average_weights = sum_scaled_weights(scaled_local_weight_list)\n    \n    #update global model \n    global_model.set_weights(average_weights)\n\n    #test global model and print out metrics after each communications round\n    for(X_test, Y_test) in test_batched:\n        global_acc, global_loss = test_model(X_test, Y_test, global_model, comm_round)\n#         list3.append(global_acc)\n#         list4.append(global_loss)","metadata":{"execution":{"iopub.status.busy":"2022-11-15T12:58:10.109162Z","iopub.execute_input":"2022-11-15T12:58:10.109751Z","iopub.status.idle":"2022-11-15T13:01:25.127010Z","shell.execute_reply.started":"2022-11-15T12:58:10.109714Z","shell.execute_reply":"2022-11-15T13:01:25.125954Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"comm_round: 0 | global_acc: 100.000% | global_loss: 0.0\ncomm_round: 1 | global_acc: 100.000% | global_loss: 0.0\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}